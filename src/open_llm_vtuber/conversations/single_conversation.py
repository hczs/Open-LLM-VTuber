import asyncio
import json
from datetime import datetime
from typing import Any, Dict, List, Optional, Union

import numpy as np
from loguru import logger

from ..agent.input_types import StrInput

# Import necessary types from agent outputs
from ..agent.output_types import AudioOutput, SentenceOutput
from ..chat_history_manager import store_message
from ..service_context import AgentInterface, ServiceContext
from .conversation_utils import (
    EMOJI_LIST,
    cleanup_conversation,
    create_batch_input,
    detect_wake_word,
    finalize_conversation_turn,
    process_agent_output,
    process_user_input,
    send_conversation_start_signals,
)
from .tts_manager import DisplayText, TTSTaskManager
from .types import WebSocketSend

system_intent_template = """
ä½ æ˜¯ä¸€ä¸ªæ„å›¾è¯†åˆ«åŠ©æ‰‹ï¼Œéœ€è¦å°†ç”¨æˆ·è¾“å…¥çš„è‡ªç„¶è¯­è¨€è¯†åˆ«ä¸ºæ˜ç¡®çš„æ„å›¾ï¼Œå¹¶ä¸¥æ ¼è¿”å› JSON æ ¼å¼ç»“æœã€‚

è¦æ±‚ï¼š
1. å¦‚æœç”¨æˆ·åªæ˜¯é—²èŠï¼ˆä¾‹å¦‚æ‰“æ‹›å‘¼ã€é—®å¤©æ°”ã€é—®æ•°å­—äººæ„Ÿå—ï¼‰ï¼Œè¯·è¿”å›ï¼š
{
  "intent": "chat"
}

2. å¦‚æœç”¨æˆ·çš„è¾“å…¥æ¶‰åŠæ‰§è¡Œå‘½ä»¤ï¼ˆæ‰“å¼€è¯¾ç¨‹ï¼‰ï¼Œè¯·è¿”å›ï¼š
{
  "intent": "command",
  "action": "<åŠ¨ä½œåç§°: æ‰“å¼€è¯¾ç¨‹ -> open_course>",
  "index": <åºå·,è¯¾ç¨‹index,ä¸‹æ ‡,ä»0å¼€å§‹è®¡ç®—>,
  "msg": "<æ•°å­—äººéœ€è¦å¯¹ç”¨æˆ·è¯´çš„è¯>"
}

3.å¦‚æœç”¨æˆ·çš„é—®é¢˜æ˜¯**å‘½ä»¤**,ä½†æ˜¯ç”¨æˆ·å‘½ä»¤ä¸æ˜¯**æ‰“å¼€è¯¾ç¨‹**,è€Œæ˜¯**å…¶ä»–å‘½ä»¤**, è¯·è¿”å›:
{
    "intent": "unknown"
}

4.å¦‚æœç”¨æˆ·çš„å‘½ä»¤å±äº"æ‰“å¼€è¯¾ç¨‹",ä½†æ˜¯æè¿°ä¸å¤Ÿå®Œæ•´(æ¯”å¦‚è¯´ä¸çŸ¥é“ç¬¬å‡ ä¸ªè¯¾ç¨‹,è¯¾ç¨‹åç§°æè¿°ä¸æ¸…)ï¼Œè¯·è¿”å›:
{
    "intent": "improve",
    "msg": "<å…·ä½“éœ€è¦è¡¥å……çš„å†…å®¹(æ¯”å¦‚è¯´éœ€è¦ç¡®è®¤è¯¾ç¨‹åç§°,éœ€è¦ç¡®è®¤è¯¾ç¨‹åºå·),ä¾‹å¦‚: æ‚¨å¥½,æ²¡æœ‰ç†è§£æ‚¨çš„æ„æ€,è¯·æ‚¨æè¿°çš„æ›´å…·ä½“ä¸€äº›>"
} 

çº¦æŸï¼š
- å¿…é¡»è¾“å‡ºåˆæ³• JSONï¼Œä¸è¦æœ‰å¤šä½™è§£é‡Šã€‚
- "index" ä¸ºæ•´æ•°,ä¸èƒ½ä¸ºç©º.
- "msg" è¦æ˜¯è‡ªç„¶çš„ä¸­æ–‡å›ç­”ã€‚
- ä½ ç°åœ¨åªèƒ½æ¥å—"æ‰“å¼€è¯¾ç¨‹"çš„å‘½ä»¤ï¼Œå…¶ä»–å‘½ä»¤éƒ½è¿”å›"unknown"

ä¸‹é¢æ˜¯ä¸€äº›ä¾‹å­ï¼š

ç”¨æˆ·è¾“å…¥: "ä½ å¥½å•Š"
è¿”å›:
{
  "intent": "chat"
}

ç”¨æˆ·è¾“å…¥: "å¸®æˆ‘æ‰“å¼€æ¨èè¯¾ç¨‹ä¸­çš„ç¬¬äºŒä¸ªè§†é¢‘"
è¿”å›:
{
  "intent": "command",
  "action": "open_course",
  "index": 1,
  "msg": "å¥½çš„ï¼Œæˆ‘ç°åœ¨ä¸ºæ‚¨æ‰“å¼€ç¬¬äºŒä¸ªè§†é¢‘"
}

ç”¨æˆ·è¾“å…¥: "å¸®æˆ‘æ‰“å¼€è®¤è¯†ä¼ å‡ºç¥ç»ç³»ç»Ÿè¯ç‰©è¯¾ç¨‹"
è¿”å›:
{
  "intent": "command",
  "action": "open_course",
  "index": 1,
  "msg": "å¥½çš„ï¼Œæˆ‘ç°åœ¨ä¸ºæ‚¨æ‰“å¼€è®¤è¯†ä¼ å‡ºç¥ç»ç³»ç»Ÿè¯ç‰©è¯¾ç¨‹è§†é¢‘"
}

ç”¨æˆ·è¾“å…¥: "å¸®æˆ‘æ‰“å¼€æ¨èè¯¾ç¨‹ä¸­çš„è¯¾ç¨‹"
è¿”å›:
{
  "intent": "improve",
  "msg": "æ‚¨å¥½ï¼Œæ²¡æœ‰ç†è§£æ‚¨çš„æ„æ€ï¼Œè¯·æ‚¨æè¿°çš„æ›´å…·ä½“ä¸€äº›"
}

æ‰€æœ‰è¯¾ç¨‹ä¿¡æ¯: <context>

"""

user_prompt = """
ç”¨æˆ·é—®é¢˜: <question>
"""

course_json = """
[
  {
    "index": 0,
    "title": "è®¤è¯†è¯ç‰©å­¦",
    "imageIndex": 1,
    "type": "è§†é¢‘è¯¾ç¨‹",
    "tags": ["è¯ç‰©åŸºç¡€", "è¯ç‰©å­¦", "åŸºç¡€æ¦‚å¿µ"],
    "description": "æŒæ¡è¯ç‰©ã€è¯ç‰©å­¦ã€è¯æ•ˆå­¦ã€è¯åŠ¨å­¦çš„æ¦‚å¿µåŠç›¸äº’å…³ç³»ï¼Œäº†è§£è¯ç‰©å­¦å‘å±•å²ã€‚",
    "courseTime": "0.14å°æ—¶",
    "compatibility": null,
    "level": null
  },
  {
    "index": 1,
    "title": "è®¤è¯†ä¼ å‡ºç¥ç»ç³»ç»Ÿè¯ç‰©",
    "imageIndex": 2,
    "type": "è§†é¢‘è¯¾ç¨‹",
    "tags": ["ç¥ç»ç³»ç»Ÿ", "ç”Ÿç†æ•ˆåº”", "ç¥ç»è¯ç‰©"],
    "description": "æŒæ¡ä¼ å‡ºç¥ç»ç³»ç»Ÿé€’è´¨ã€å—ä½“å’Œä¸»è¦ç”Ÿç†æ•ˆåº”ï¼Œç†Ÿæ‚‰ä¼ å‡ºç¥ç»ç³»ç»Ÿè¯ç‰©çš„åˆ†ç±»åŸåˆ™ï¼Œäº†è§£ä¼ å‡ºç¥ç»çš„åˆ†ç±»å’Œç‰¹ç‚¹ã€‚",
    "courseTime": "0.15å°æ—¶",
    "compatibility": null,
    "level": null
  },
  {
    "index": 2,
    "title": "é•‡é™å‚¬çœ è¯",
    "imageIndex": 3,
    "type": "è§†é¢‘è¯¾ç¨‹",
    "tags": ["ä¸­æ¢ç¥ç»", "é•‡é™", "å‚¬çœ è¯"],
    "description": "æŒæ¡è‹¯äºŒæ°®ä“¬ç±»è¯ç‰©çš„ä½œç”¨å’Œç”¨é€”ã€ä¸è‰¯ååº”ä»¥åŠç”¨è¯æŒ‡å¯¼ï¼Œç†Ÿæ‚‰å·´æ¯”å¦¥ç±»è¯ç‰©çš„ä¸»è¦ç‰¹ç‚¹å’Œç”¨è¯æŒ‡å¯¼ï¼Œäº†è§£å…¶ä»–å¸¸ç”¨é•‡é™å‚¬çœ è¯çš„ä¸»è¦ç‰¹ç‚¹ã€‚",
    "courseTime": "0.16å°æ—¶",
    "compatibility": null,
    "level": null
  },
  {
    "index": 3,
    "title": "æŠ—é«˜è¡€å‹è¯",
    "imageIndex": 4,
    "type": "è§†é¢‘è¯¾ç¨‹",
    "tags": ["ä¸­æ¢ç¥ç»", "æŠ—é«˜è¡€å‹", "é«˜è¡€å‹"],
    "description": "æŒæ¡ä¸€çº¿æŠ—é«˜è¡€å‹è¯çš„ä¸»è¦ç‰¹ç‚¹å’Œç”¨è¯æŒ‡å¯¼ï¼Œç†Ÿæ‚‰é«˜è¡€å‹è¯çš„ä½œç”¨ç¯èŠ‚å’Œç±»åˆ«ï¼Œäº†è§£å…¶ä»–æŠ—é«˜è¡€å‹è¯çš„ä¸»è¦ç‰¹ç‚¹ã€‚",
    "courseTime": "0.12å°æ—¶",
    "compatibility": null,
    "level": null
  },
  {
    "index": 4,
    "title": "å½±å“ç”²çŠ¶è…ºåŠŸèƒ½çš„è¯ç‰©",
    "imageIndex": 5,
    "type": "è§†é¢‘è¯¾ç¨‹",
    "tags": ["å†…åˆ†æ³Œ", "ç”²çŠ¶è…º", "æ¿€ç´ "],
    "description": "æŒæ¡æŠ—ç”²çŠ¶è…ºè¯çš„ç§ç±»ã€ä½œç”¨å’Œç”¨é€”ã€ä¸è‰¯ååº”åŠç”¨è¯æŒ‡å¯¼ï¼Œç†Ÿæ‚‰æˆ–äº†è§£ç”²çŠ¶è…ºæ¿€ç´ çš„ä¸»è¦ç‰¹ç‚¹å’Œç”¨è¯æŒ‡å¯¼ã€‚",
    "courseTime": "0.12å°æ—¶",
    "compatibility": null,
    "level": null
  },
  {
    "index": 5,
    "title": "å‘¼å¸ç³»ç»Ÿè¯ç‰©",
    "imageIndex": 6,
    "type": "è§†é¢‘è¯¾ç¨‹",
    "tags": ["å†…è„ç³»ç»Ÿ", "å¹³å–˜è¯", "å‘¼å¸ç³»ç»Ÿ"],
    "description": "æŒæ¡å¹³å–˜è¯çš„ç§ç±»ã€ä¸»è¦ç‰¹ç‚¹å’Œç”¨è¯æŒ‡å¯¼ï¼Œç†Ÿæ‚‰é•‡å’³è¯ã€ç¥›ç—°è¯çš„ç§ç±»ã€ä¸»è¦ç‰¹ç‚¹å’Œç”¨è¯æŒ‡å¯¼ï¼Œäº†è§£è¯ç‰©é•‡å’³ã€ç¥›ç—°ã€å¹³å–˜çš„ä½œç”¨æœºåˆ¶å’Œå¸¸ç”¨å¤æ–¹åˆ¶å‰‚ã€‚",
    "courseTime": "0.14å°æ—¶",
    "compatibility": null,
    "level": null
  }
]
"""

system_intent_template = system_intent_template.replace("<context>", course_json)


async def intention_recognition(
    input_text: str, agent_engine: AgentInterface, from_name: str
):
    input_text = user_prompt.replace("<question>", input_text)
    str_input = StrInput(system=system_intent_template, user=input_text)
    return await agent_engine.chat_full(str_input)


async def process_single_conversation(
    context: ServiceContext,
    websocket_send: WebSocketSend,
    client_uid: str,
    user_input: Union[str, np.ndarray],
    images: Optional[List[Dict[str, Any]]] = None,
    session_emoji: str = np.random.choice(EMOJI_LIST),
    metadata: Optional[Dict[str, Any]] = None,
) -> str:
    """Process a single-user conversation turn

    Args:
        context: Service context containing all configurations and engines
        websocket_send: WebSocket send function
        client_uid: Client unique identifier
        user_input: Text or audio input from user
        images: Optional list of image data
        session_emoji: Emoji identifier for the conversation
        metadata: Optional metadata for special processing flags

    Returns:
        str: Complete response text
    """
    # Create TTSTaskManager for this conversation
    tts_manager = TTSTaskManager()
    full_response = ""  # Initialize full_response here

    try:
        # æ¬¢è¿è¯
        if metadata and metadata["msg_type"] == "text-input" and user_input == "start":
            await send_conversation_start_signals(websocket_send)
            # æ’­æ”¾æ¬¢è¿è¯
            welcome_speech = context.system_config.welcome_speech
            # æ ¹æ®å½“å‰æ—¶é—´æ›¿æ¢æ¬¢è¿è¯ä¸­çš„time_greeting
            if "{time_greeting}" in welcome_speech:
                current_hour = datetime.now().hour
                if 5 <= current_hour < 12:
                    time_greeting = "ä¸Šåˆå¥½"
                elif 12 <= current_hour < 18:
                    time_greeting = "ä¸‹åˆå¥½"
                else:
                    time_greeting = "æ™šä¸Šå¥½"
                welcome_speech = welcome_speech.replace(
                    "{time_greeting}", time_greeting
                )
            logger.info(f"Playing welcome speech: {welcome_speech}")
            await tts_manager.speak(
                tts_text=welcome_speech,
                display_text=DisplayText(text=welcome_speech),
                live2d_model=context.live2d_model,
                tts_engine=context.tts_engine,
                websocket_send=websocket_send,
                actions=None,
            )
            # if tts_manager.task_list:
            #     await asyncio.gather(*tts_manager.task_list)
            #     await websocket_send(json.dumps({"type": "backend-synth-complete"}))
            await finalize_conversation_turn(
                tts_manager=tts_manager,
                websocket_send=websocket_send,
                client_uid=client_uid,
            )
            return ""
        # å”¤é†’è¯æ£€æµ‹
        wake, input_text = await detect_wake_word(
            user_input=user_input,
            asr_engine=context.asr_engine,
            wakeup_words=context.system_config.wakeup_words,
            websocket_send=websocket_send,
        )
        logger.info(f"user question: {input_text}")
        if not wake:
            logger.info("No wake word detected. Ignoring input.")
            await finalize_conversation_turn(
                tts_manager=tts_manager,
                websocket_send=websocket_send,
                client_uid=client_uid,
            )
            return ""

        # æ„å›¾è¯†åˆ«
        # intent = await intention_recognition(
        #     input_text=input_text,
        #     agent_engine=context.agent_engine,
        #     from_name=context.character_config.human_name,
        # )

        # logger.info(f"Intent recognition result: {intent}")

        # if isinstance(intent, str):
        #     intent = extract_json(text=intent)
        #     logger.info(f"Extracted intent: {intent}")

        # next_step = "chat"
        # if (
        #     intent is None
        #     or not isinstance(intent, dict)
        #     or intent["intent"] == "chat"
        #     or intent["intent"] == "unknown"
        # ):
        #     logger.warning("Failed to extract valid intent JSON. Proceeding as chat.")
        #     next_step = "chat"
        # else:
        #     next_step = intent["intent"]

        # if (
        #     next_step == "improve"
        #     and isinstance(intent, dict)
        #     and intent["msg"] is not None
        # ):
        #     # send audio msg
        #     await tts_manager.speak(
        #         tts_text=intent["msg"],
        #         display_text=DisplayText(text=intent["msg"]),
        #         live2d_model=context.live2d_model,
        #         tts_engine=context.tts_engine,
        #         websocket_send=websocket_send,
        #         actions=None,
        #     )
        #     # Wait for any pending TTS tasks
        #     if tts_manager.task_list:
        #         await asyncio.gather(*tts_manager.task_list)
        #         await websocket_send(json.dumps({"type": "backend-synth-complete"}))
        #     return ""
        # elif (
        #     next_step == "command"
        #     and isinstance(intent, dict)
        #     and intent["msg"] is not None
        # ):
        #     intent["className"] = "card-block"
        #     # send audio msg
        #     await tts_manager.speak(
        #         tts_text=intent["msg"],
        #         display_text=DisplayText(text=intent["msg"]),
        #         live2d_model=context.live2d_model,
        #         tts_engine=context.tts_engine,
        #         websocket_send=websocket_send,
        #         actions=None,
        #         on_complete=lambda: websocket_send(
        #             json.dumps(
        #                 {
        #                     "type": "command",
        #                     "data": json.dumps(intent, ensure_ascii=False),
        #                 }
        #             )
        #         ),
        #     )
        #     # Wait for any pending TTS tasks
        #     if tts_manager.task_list:
        #         await asyncio.gather(*tts_manager.task_list)
        #         await websocket_send(json.dumps({"type": "backend-synth-complete"}))
        #     return ""

        # Send initial signals
        await send_conversation_start_signals(websocket_send)
        logger.info(f"New Conversation Chain {session_emoji} started!")

        # Process user input
        input_text = await process_user_input(
            input_text, context.asr_engine, websocket_send
        )
        # Create batch input
        batch_input = create_batch_input(
            input_text=input_text,
            images=images,
            from_name=context.character_config.human_name,
            metadata=metadata,
        )

        # Store user message (check if we should skip storing to history)
        skip_history = metadata and metadata.get("skip_history", False)
        if context.history_uid and not skip_history:
            store_message(
                conf_uid=context.character_config.conf_uid,
                history_uid=context.history_uid,
                role="human",
                content=input_text,
                name=context.character_config.human_name,
            )

        if skip_history:
            logger.debug("Skipping storing user input to history (proactive speak)")

        logger.info(f"User input: {input_text}")
        if images:
            logger.info(f"With {len(images)} images")

        try:
            # agent.chat yields Union[SentenceOutput, Dict[str, Any]]
            agent_output_stream = context.agent_engine.chat(batch_input)

            async for output_item in agent_output_stream:
                if (
                    isinstance(output_item, dict)
                    and output_item.get("type") == "tool_call_status"
                ):
                    # Handle tool status event: send WebSocket message
                    output_item["name"] = context.character_config.character_name
                    logger.debug(f"Sending tool status update: {output_item}")

                    await websocket_send(json.dumps(output_item))

                elif isinstance(output_item, (SentenceOutput, AudioOutput)):
                    # Handle SentenceOutput or AudioOutput
                    response_part = await process_agent_output(
                        output=output_item,
                        character_config=context.character_config,
                        live2d_model=context.live2d_model,
                        tts_engine=context.tts_engine,
                        websocket_send=websocket_send,  # Pass websocket_send for audio/tts messages
                        tts_manager=tts_manager,
                        translate_engine=context.translate_engine,
                    )
                    # Ensure response_part is treated as a string before concatenation
                    response_part_str = (
                        str(response_part) if response_part is not None else ""
                    )
                    full_response += response_part_str  # Accumulate text response
                else:
                    logger.warning(
                        f"Received unexpected item type from agent chat stream: {type(output_item)}"
                    )
                    logger.debug(f"Unexpected item content: {output_item}")

        except Exception as e:
            logger.exception(
                f"Error processing agent response stream: {e}"
            )  # Log with stack trace
            await websocket_send(
                json.dumps(
                    {
                        "type": "error",
                        "message": f"Error processing agent response: {str(e)}",
                    }
                )
            )
            # full_response will contain partial response before error
        # --- End processing agent response ---

        # Wait for any pending TTS tasks
        if tts_manager.task_list:
            await asyncio.gather(*tts_manager.task_list)
            await websocket_send(json.dumps({"type": "backend-synth-complete"}))

        await finalize_conversation_turn(
            tts_manager=tts_manager,
            websocket_send=websocket_send,
            client_uid=client_uid,
        )

        if context.history_uid and full_response:  # Check full_response before storing
            store_message(
                conf_uid=context.character_config.conf_uid,
                history_uid=context.history_uid,
                role="ai",
                content=full_response,
                name=context.character_config.character_name,
                avatar=context.character_config.avatar,
            )
            logger.info(f"AI response: {full_response}")

        return full_response  # Return accumulated full_response

    except asyncio.CancelledError:
        logger.info(f"ğŸ¤¡ğŸ‘ Conversation {session_emoji} cancelled because interrupted.")
        raise
    except Exception as e:
        logger.error(f"Error in conversation chain: {e}")
        await websocket_send(
            json.dumps({"type": "error", "message": f"Conversation error: {str(e)}"})
        )
        raise
    finally:
        cleanup_conversation(tts_manager, session_emoji)
